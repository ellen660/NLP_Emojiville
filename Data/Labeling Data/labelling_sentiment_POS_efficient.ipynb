{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from openai import OpenAI\n",
    "import pandas as pd\n",
    "from concurrent.futures import ProcessPoolExecutor\n",
    "import ast\n",
    "\n",
    "client = OpenAI(api_key='PUT API KEY HERE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv('C:/Users/joyse/Desktop/6.8610/Research/Emojiville/Data Processing/Clean Dataset/emojify_cleaned_10k.csv')\n",
    "\n",
    "examples_df = pd.read_csv('C:/Users/joyse/Desktop/6.8610/Research/Emojiville/Data Processing/Clean Dataset/manually_labelled_cleaned_30.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate example instructions dynamically\n",
    "example_instructions = \"Here are some examples to guide your labeling process:\\n\\n\"\n",
    "\n",
    "for _, row in examples_df.iterrows():\n",
    "    # Format each example into the prompt\n",
    "    example_instructions += (\n",
    "        f\"Tweet: {row['Tokens']}\\n\"\n",
    "        f\"Sentiment_score: {row['Sentiment_score']}\\n\"\n",
    "        f\"Sentiment_emotion: {row['Sentiment_emotion']}\\n\"\n",
    "        f\"Part_of_speech: {row['Part_of_speech']}\\n\\n\"\n",
    "    )\n",
    "\n",
    "# Combine examples with the main system prompt\n",
    "sentiment_system_prompt = f'''\n",
    "Your goal is to extract the sentiment score, emotional category, and part of speech for the emoji in a tokenized tweet. Below are examples to guide your labeling process:\n",
    "\n",
    "{example_instructions}\n",
    "\n",
    "**Instructions:**\n",
    "\n",
    "- **Step 1**: Identify the emoji in the tokenized tweet. Emojis are represented as symbols or non-English characters.\n",
    "- **Step 2**: For the emoji, use the emoji and the context of the language in the tweet to determine:\n",
    "    - **Sentiment**: An integer where **1** is positive, **-1** is negative, and **0** is neutral.\n",
    "    - **Emotion**: One or more emotions from [\"joy\", \"sadness\", \"anger\", \"fear\", \"love\", \"surprise\", \"disgust\"].\n",
    "    - **Part of Speech (POS)**: One of the 15 UD POS tags that best represents the emoji in the context of the tweet. POS tags include:\n",
    "        - **NOUN**: Names of people, places, things, or ideas (e.g., \"dog\", \"house\").\n",
    "        - **VERB**: Words representing actions or states of being (e.g., \"run\", \"think\").\n",
    "        - **ADJ**: Words describing or modifying nouns (e.g., \"big\", \"blue\").\n",
    "        - **ADV**: Words describing verbs, adjectives, or other adverbs (e.g., \"quickly\", \"very\").\n",
    "        - **PRON**: Words that replace nouns or noun phrases (e.g., \"he\", \"they\").\n",
    "        - **PROPN**: Specific names of entities (e.g., \"London\", \"Google\").\n",
    "        - **DET**: Words that specify nouns (e.g., \"the\", \"an\").\n",
    "        - **ADP**: Prepositions or postpositions (e.g., \"in\", \"on\").\n",
    "        - **CCONJ**: Coordinating conjunctions joining words or clauses (e.g., \"and\", \"but\").\n",
    "        - **SCONJ**: Subordinating conjunctions joining dependent clauses (e.g., \"because\", \"although\").\n",
    "        - **AUX**: Helper verbs used with main verbs (e.g., \"is\", \"will\").\n",
    "        - **NUM**: Words expressing numbers or quantities (e.g., \"one\", \"100\").\n",
    "        - **PART**: Function words like \"to\" (in \"to run\") or \"not\".\n",
    "        - **PUNCT**: Punctuation symbols (e.g., \".\", \",\", \"!\").\n",
    "        - **INTJ**: Words expressing exclamations (e.g., \"wow\", \"oops\").\n",
    "\n",
    "\n",
    "- **Step 3**: Provide the output in the following JSON format:\n",
    "\n",
    "```json\n",
    "{{\n",
    "    \"sentiment\": int,     // Int of sentiment for the emoji.\n",
    "    \"emotion\": string,    // Str of emotion for the emoji.\n",
    "    \"pos\": string         // Str of POS tag for the emoji.\n",
    "}}\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the new data\n",
    "\n",
    "def get_labels(description):\n",
    "    response = client.chat.completions.create(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    temperature=0.1,\n",
    "    # This is to enable JSON mode, making sure responses are valid json objects\n",
    "    response_format={ \n",
    "        \"type\": \"json_object\"\n",
    "    },\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": sentiment_system_prompt\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": description\n",
    "        }\n",
    "    ],\n",
    "    )\n",
    "    return response.choices[0].message.content\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_batch(batch):\n",
    "    results = []\n",
    "    for index, row in batch.iterrows():\n",
    "        for attempt in range(3):  # Retry up to 3 times\n",
    "            try:\n",
    "                # Get the tokenized tweet\n",
    "                tweet = row['tokens']\n",
    "\n",
    "                # Call the GPT API\n",
    "                gpt_response = json.loads(get_labels(tweet))\n",
    "\n",
    "                # Extract sentiment, emotion, and POS\n",
    "                sentiment = gpt_response['sentiment']\n",
    "                emotion = gpt_response['emotion']\n",
    "                pos = gpt_response['pos']\n",
    "\n",
    "                # Append results\n",
    "                results.append({\n",
    "                    'Tokens': tweet,\n",
    "                    'Sentiment_score': sentiment,\n",
    "                    'Sentiment_emotion': emotion,\n",
    "                    'Part_of_speech': pos\n",
    "                })\n",
    "                break  # Exit retry loop on success\n",
    "            except Exception as e:\n",
    "                if attempt < 2:  # Log only for retryable attempts\n",
    "                    print(f\"Retrying tweet at index {index}: {e}\")\n",
    "                else:\n",
    "                    print(f\"Failed to process tweet at index {index}: {e}\")\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from concurrent.futures import ThreadPoolExecutor\n",
    "import tqdm  # For progress tracking\n",
    "\n",
    "def label_tweets_parallel(df, num_workers=8, batch_size=50):\n",
    "    \"\"\"\n",
    "    Labels tweets using GPT API with parallel and batched processing.\n",
    "\n",
    "    Args:\n",
    "    - df (pd.DataFrame): The input DataFrame containing tweets.\n",
    "    - num_workers (int): Number of parallel threads.\n",
    "    - batch_size (int): Number of tweets in each batch.\n",
    "\n",
    "    Returns:\n",
    "    - pd.DataFrame: DataFrame with labeled results.\n",
    "    \"\"\"\n",
    "    # Split the DataFrame into smaller batches\n",
    "    batches = [df.iloc[i:i + batch_size] for i in range(0, len(df), batch_size)]\n",
    "\n",
    "    # Use ThreadPoolExecutor for parallel processing\n",
    "    results = []\n",
    "    with ThreadPoolExecutor(max_workers=num_workers) as executor:\n",
    "        # Process each batch and track progress\n",
    "        for batch_result in tqdm.tqdm(executor.map(process_batch, batches), total=len(batches), desc=\"Processing Batches\"):\n",
    "            results.extend(batch_result)\n",
    "\n",
    "    # Convert results to a DataFrame\n",
    "    return pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1000/1000 [07:38<00:00,  2.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Labeling complete!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "\n",
    "      # Label the dataset in parallel\n",
    "    labeled_df = label_tweets_parallel(dataset, num_workers=16, batch_size=10)\n",
    "\n",
    "    print(\"Labeling complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tokens</th>\n",
       "      <th>Sentiment_score</th>\n",
       "      <th>Sentiment_emotion</th>\n",
       "      <th>Part_of_speech</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>['if', 'not', 'later', ',', 'when', '?', 'üçë']</td>\n",
       "      <td>1</td>\n",
       "      <td>Joy</td>\n",
       "      <td>NOUN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>['üîó', '|', 'the', 'izombie', '4x08', 'page', 'is', 'now', 'up', '!', 'containing', 'trivia', ',', 'quotes', ',', 'reviews', ',', 'a', 'look', 'at', \"liv's\", 'style', ',', 'comic', 'slides', 'gifs', ',', 'the', 'b', '‚Ä¶']</td>\n",
       "      <td>0</td>\n",
       "      <td>Surprise</td>\n",
       "      <td>NOUN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>['hearing', 'bts', 'at', 'work', 'still', 'amazes', 'me', 'üòç']</td>\n",
       "      <td>1</td>\n",
       "      <td>Love</td>\n",
       "      <td>VERB</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>['ayyy', 'this', 'is', 'lit', 'üî•']</td>\n",
       "      <td>1</td>\n",
       "      <td>Joy</td>\n",
       "      <td>ADJ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>['well', 'richard', 'i', 'jumped', 'in', 'the', 'shower', 'and', 'saved', 'her', 'so', 'back', 'off', 'üò°']</td>\n",
       "      <td>-1</td>\n",
       "      <td>Anger</td>\n",
       "      <td>ADJ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>[\"that's\", 'why', 'we', 'go', 'to', 'the', \"'\", 'rewind', \"'\", 'festival', 'these', 'days', 'üòÇ']</td>\n",
       "      <td>1</td>\n",
       "      <td>Joy</td>\n",
       "      <td>VERB</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>['good', 'morning', 'everyone', 'üòÉ']</td>\n",
       "      <td>1</td>\n",
       "      <td>Joy</td>\n",
       "      <td>ADJ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>['i', \"don't\", 'care', 'about', 'people', 'who', 'judge', 'me', '.', \"i'am\", 'just', 'gonna', 'do', 'whatever', 'makes', 'me', 'happy', 'in', 'my', 'life', 'üòé']</td>\n",
       "      <td>1</td>\n",
       "      <td>Joy</td>\n",
       "      <td>ADJ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>['you', 'know', 'what', 'would', 'be', 'the', 'hottest', 'valentine', 'date', 'for', 'armys', '?', 'hixtape', 'üî•', 'kiddin', '‚Ä¶']</td>\n",
       "      <td>1</td>\n",
       "      <td>Joy</td>\n",
       "      <td>NOUN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>['when', 'your', 'friend', 'reminds', 'you', 'about', 'the', 'new', 'on', 'tonight', '.', 'üíØ']</td>\n",
       "      <td>1</td>\n",
       "      <td>Joy</td>\n",
       "      <td>ADJ</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows √ó 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                         Tokens  \\\n",
       "0                                                                                                                                                                                 ['if', 'not', 'later', ',', 'when', '?', 'üçë']   \n",
       "1   ['üîó', '|', 'the', 'izombie', '4x08', 'page', 'is', 'now', 'up', '!', 'containing', 'trivia', ',', 'quotes', ',', 'reviews', ',', 'a', 'look', 'at', \"liv's\", 'style', ',', 'comic', 'slides', 'gifs', ',', 'the', 'b', '‚Ä¶']   \n",
       "2                                                                                                                                                                ['hearing', 'bts', 'at', 'work', 'still', 'amazes', 'me', 'üòç']   \n",
       "3                                                                                                                                                                                            ['ayyy', 'this', 'is', 'lit', 'üî•']   \n",
       "4                                                                                                                    ['well', 'richard', 'i', 'jumped', 'in', 'the', 'shower', 'and', 'saved', 'her', 'so', 'back', 'off', 'üò°']   \n",
       "..                                                                                                                                                                                                                          ...   \n",
       "95                                                                                                                             [\"that's\", 'why', 'we', 'go', 'to', 'the', \"'\", 'rewind', \"'\", 'festival', 'these', 'days', 'üòÇ']   \n",
       "96                                                                                                                                                                                         ['good', 'morning', 'everyone', 'üòÉ']   \n",
       "97                                                             ['i', \"don't\", 'care', 'about', 'people', 'who', 'judge', 'me', '.', \"i'am\", 'just', 'gonna', 'do', 'whatever', 'makes', 'me', 'happy', 'in', 'my', 'life', 'üòé']   \n",
       "98                                                                                            ['you', 'know', 'what', 'would', 'be', 'the', 'hottest', 'valentine', 'date', 'for', 'armys', '?', 'hixtape', 'üî•', 'kiddin', '‚Ä¶']   \n",
       "99                                                                                                                               ['when', 'your', 'friend', 'reminds', 'you', 'about', 'the', 'new', 'on', 'tonight', '.', 'üíØ']   \n",
       "\n",
       "    Sentiment_score Sentiment_emotion Part_of_speech  \n",
       "0                 1               Joy           NOUN  \n",
       "1                 0          Surprise           NOUN  \n",
       "2                 1              Love           VERB  \n",
       "3                 1               Joy            ADJ  \n",
       "4                -1             Anger            ADJ  \n",
       "..              ...               ...            ...  \n",
       "95                1               Joy           VERB  \n",
       "96                1               Joy            ADJ  \n",
       "97                1               Joy            ADJ  \n",
       "98                1               Joy           NOUN  \n",
       "99                1               Joy            ADJ  \n",
       "\n",
       "[100 rows x 4 columns]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "labeled_df.head(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_path = \"C:/Users/joyse/Desktop/6.8610/Research/Emojiville/Data Processing/Clean Dataset\"\n",
    "filename1 = 'emojify_cleaned_10k_labelled.csv'\n",
    "\n",
    "file_path1 = os.path.join(folder_path, filename1)\n",
    "\n",
    "labeled_df.to_csv(file_path1, index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
